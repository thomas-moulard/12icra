\section{Stereo Simultaneous Localization and Mapping}\label{sec:vslam}
In this section, we briefly review the main components of our stereo
visual SLAM system. Notice here that we are interested in using visual
SLAM for building a persistent 3D map of the environment that can be
used later for vision-based localization and planning~\PFA{is planning
  the best word here?} purposes. We employ the two cameras that are
attached to the ears of the HRP-2 robot. These two cameras have a
baseline of approximately 14.4~cm and an horizontal field of view of
$90^{\circ}$ for each of the cameras.

Prior to any SLAM processing, the stereo rig is calibrated obtaining
the intrinsics parameters of each of the cameras and the extrinsics
parameters between them. Once we have obtained the intrinsics and
extrinsics of the stereo rig, we can correct the distortion of the
images and perform stereo rectification~\citep{Hartley99ijcv}. Stereo
rectification simplifies considerably the stereo correspondences
problem and allows to compute dense disparity or depth maps.

Our visual SLAM system combines accurate relative camera pose
estimation by means of visual odometry~\cite{Kaess09icra} and a
hierarchical optimization of the motion and the structure by means of
local bundle adjustment~\cite{Mouragnon09ivc}.

\subsection{Stereo Visual Odometry}\label{sec:visual_odometry}

We estimate the relative camera motion between consecutive frames by
matching the set of correspondences between two frames. The set of 2D
features are detected by means of the well-known Harris corner
detector~\cite{Harris88avc} at the original image resolution. We
detect features only for the left image of the stereo pair. Then, we
find the correspondences of the 2D features in the right image by
accessing the disparity map. At the end, what we have is a set of
stereo features
$\mathcal{F}_{t}=\left\{\left(u_{L},u_{R},v\right)_{i}\right\}$, where
$\left(u_{L},v\right)$ is the location of the feature in the left
image and $\left(u_{R},v\right)$ is the corresponding location in the
right image. In addition, we also store for each stereo feature
$\mathcal{F}_{t}$ the coordinates of the i-th reconstructed 3D point
$h_{i,t}=\left(x \ y \ z\right)^{t}$ with respect to the camera
coordinate frame at that time instant $t$.

For each detected 2D feature in the left image we also extract a
descriptor vector that encodes its appearance information. Similar to
Speeded Up Robust Features (SURF)~\cite{Bay08cviu}, for a detected
feature at a certain scale, we compute a unitary descriptor vector of
dimension $16$ in order to speed up the descriptor computation. We use
the upright version of the descriptors (no invariance to rotation)
since upright descriptors perform better in scenarios where the camera
only rotates around its vertical axis, which is often the case of
humanoid robots. For simplicity, we do not use any kind of spatial or
Gaussian weighting.

Once we have computed the features descriptors, we find the set of
putative matches between the stereo features from the current
frame~$\mathcal{F}_{t}$ and the previous one~$\mathcal{F}_{t-1}$ by
matching their associated list of descriptors vectors. After finding
the set of putative matches between two consecutive frames we estimate
the relative camera motion using a standard two-point algorithm in a
Random Sample Consensus (RANSAC)~\cite{Bolles81ijcai} setting by
minimizing the following cost function:
%
\begin{equation} \label{eq:three_pt}
\argmin_{\textit{R}_{t-1}^{t},\mathbf{t}_{t-1}^{t}} \sum\limits_{i} \left\| z_{i,t} - \Pi\left(\textit{R}_{t-1}^{t},\mathbf{t}_{t-1}^{t},h_{i,t-1}\right)\right\|_{2}
\end{equation}
%
where $z_{i,t}=\left\{\left(u_{L},u_{R},v\right)_{i}\right\}$ are the
set of 2D measurements of a stereo feature at time t and $\Pi$ is a
function that projects a 3D point $h_{i,t-1}$ (referenced to the
camera coordinate frame at time $t-1$) to the image coordinate frame
at time $t$. This projection function $\Pi$ involves a rotation
$\textit{R}_{t-1}^{t}$ and a translation $\mathbf{t}_{t-1}^{t}$ of 3D
points between both coordinate frames and a projection onto the image
plane by means of the stereo rig calibration parameters. The resulting
relative camera motion is transformed to a global coordinate frame
(usually referenced to the first frame of the sequence) and then is
used by the mapping management module. We use the Levenberg-Marquardt
algorithm for all the nonlinear optimizations.

% MAPPING - BUNDLE ADJUSTMENT
\subsection{Bundle Adjustment}\label{sec:ba}
When the accumulated motion in translation or rotation from the visual
odometry module is higher than a fixed threshold we decide to create a
new \textit{keyframe}. This keyframe, will be optimized later in a
local bundle adjusment procedure. In the local bundle adjustment
optimization, 3D points and camera poses are refined simultaneously
through the sequence. Similar to~\cite{Mouragnon09ivc} we use a
sliding window approach taking into account the last $N$ keyframes,
optimizing only $n$ keyframes at each stage. Typical values for
$\left(N,n\right)$ are $\left(10,3\right)$ respectively.

We perform an intelligent management of features into the map, in
order to produce an equal distribution of feature locations over the
image. While adding a new feature to the map, we also store its
associated appearance descriptor and 3D point location. Then, we try
to match the feature descriptor against the detected new 2D features
on a new keyframe by matching their associated descriptors in a high
probability search area. In this way, we can create for a map element,
\textit{feature tracks} that contain the information of the 2D
measurements of the feature (both in left and right views) in several
keyframes. Then, this information is used as an input for the local
bundle adjustment procedure. Features are deleted from the map when
the mean re-projection error per frame in the 3D reconstruction is
higher than a fixed threshold (e.g. 3 pixels).

By means of appearance based methods, loop closure situations can be
detected and the residual error in the 3D reconstruction can be
corrected in a global BA procedure.
